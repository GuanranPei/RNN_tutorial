{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import time\n",
    "import csv\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128 #英文字符字典的长度，采用ASCLL码最大128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        super(NameDataset, self).__init__()\n",
    "        filename = './names_train.csv.gz' if is_train_set else './names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.len = len(self.names)\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        self.country_list = list(sorted(set(self.countries))) # set函数列表变集合，去除重复的实例\n",
    "        self.country_dict = self.getCountryDict()\n",
    "        self.country_num = len(self.country_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "    \n",
    "    def idx2country(self, index):\n",
    "        return self.country_list[index]\n",
    "    \n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainset.getCountriesNum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size) # input_size = dict_size, hidden_size = embedding_size\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional) # hidden_size = embedding_size\n",
    "        self.fc = torch.nn.Linear(hidden_size*self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        # 使用 create_tensor 确保 hidden 在模型所在设备上\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size, device=self.embedding.weight.device)\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t() # 转置将(batch, seq_len)变为(seq_len, batch)\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        hidden=self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "        # pack the input list to speed up the calculation\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding,seq_lengths)\n",
    "\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_directions==2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output=self.fc(hidden_cat)\n",
    "        return fc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2list(name):\n",
    "    arr=[ord(c) for c in name] # 将名字的string拆分成char，并转换为list\n",
    "    return arr,len(arr)\n",
    "def create_tensor(tensor):\n",
    "    return tensor\n",
    "\n",
    "def make_tensors(names,countries):\n",
    "    sequences_and_lengths = [name2list(name) for name in names] # names是一个序列，包含[name, seq_len]\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "\n",
    "    seq_tensor = torch.zeros(len(name_sequences),seq_lengths.max()).long()\n",
    "    for idx,(seq,seq_len) in enumerate(zip(name_sequences,seq_lengths),0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    seq_lengths,perm_idx=seq_lengths.sort(dim=0,descending=True) # 将原始数据按照序列长度从大到小排序的排列顺序，用于对数据进行重排序\n",
    "    seq_tensor=seq_tensor[perm_idx]\n",
    "    countries=countries[perm_idx]\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    classifier.to(device)  # 确保模型在 GPU 上\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        \n",
    "        # 数据移动到 GPU\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # 注意 seq_lengths 必须放到 CPU\n",
    "        seq_lengths = seq_lengths.to('cpu')\n",
    "\n",
    "        # 前向传播\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            end = time.time()\n",
    "            print(f'[{end - start}] Epoch {epoch}', end=' ')\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}]', end=' ')\n",
    "            print(f'loss = {total_loss / (i * len(inputs)):.4f}')\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    classifier.to(device)  # 确保模型在 GPU 上\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print(\"Evaluating trained model...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "\n",
    "            # 数据移动到 GPU\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # 注意 seq_lengths 必须放到 CPU\n",
    "            seq_lengths = seq_lengths.to('cpu')\n",
    "\n",
    "            # 前向传播\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]  # 取预测结果\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        percent = f'{100 * correct / total:.2f}'\n",
    "        print(f'Test set: Accuracy {correct}/{total} ({percent}%)')\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[0.643531322479248] Epoch 1 [2560/13374] loss = 0.0086\n",
      "[0.7757630348205566] Epoch 1 [5120/13374] loss = 0.0074\n",
      "[0.9067447185516357] Epoch 1 [7680/13374] loss = 0.0068\n",
      "[1.0405769348144531] Epoch 1 [10240/13374] loss = 0.0064\n",
      "[1.174243688583374] Epoch 1 [12800/13374] loss = 0.0060\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 4434/6700 (66.18%)\n",
      "[1.4756388664245605] Epoch 2 [2560/13374] loss = 0.0042\n",
      "[1.6048316955566406] Epoch 2 [5120/13374] loss = 0.0042\n",
      "[1.7320022583007812] Epoch 2 [7680/13374] loss = 0.0040\n",
      "[1.863473892211914] Epoch 2 [10240/13374] loss = 0.0039\n",
      "[1.9853074550628662] Epoch 2 [12800/13374] loss = 0.0038\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 4970/6700 (74.18%)\n",
      "[2.2523858547210693] Epoch 3 [2560/13374] loss = 0.0031\n",
      "[2.3745760917663574] Epoch 3 [5120/13374] loss = 0.0032\n",
      "[2.497565746307373] Epoch 3 [7680/13374] loss = 0.0031\n",
      "[2.6168582439422607] Epoch 3 [10240/13374] loss = 0.0031\n",
      "[2.7413394451141357] Epoch 3 [12800/13374] loss = 0.0030\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5222/6700 (77.94%)\n",
      "[3.024400234222412] Epoch 4 [2560/13374] loss = 0.0026\n",
      "[3.1555697917938232] Epoch 4 [5120/13374] loss = 0.0025\n",
      "[3.284473180770874] Epoch 4 [7680/13374] loss = 0.0025\n",
      "[3.4158260822296143] Epoch 4 [10240/13374] loss = 0.0025\n",
      "[3.549529790878296] Epoch 4 [12800/13374] loss = 0.0025\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5373/6700 (80.19%)\n",
      "[3.8326621055603027] Epoch 5 [2560/13374] loss = 0.0023\n",
      "[3.9578933715820312] Epoch 5 [5120/13374] loss = 0.0024\n",
      "[4.08857536315918] Epoch 5 [7680/13374] loss = 0.0023\n",
      "[4.218093633651733] Epoch 5 [10240/13374] loss = 0.0022\n",
      "[4.34318995475769] Epoch 5 [12800/13374] loss = 0.0022\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5438/6700 (81.16%)\n",
      "[4.6206676959991455] Epoch 6 [2560/13374] loss = 0.0020\n",
      "[4.745612859725952] Epoch 6 [5120/13374] loss = 0.0020\n",
      "[4.875412940979004] Epoch 6 [7680/13374] loss = 0.0020\n",
      "[5.004051685333252] Epoch 6 [10240/13374] loss = 0.0020\n",
      "[5.125887870788574] Epoch 6 [12800/13374] loss = 0.0020\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5545/6700 (82.76%)\n",
      "[5.405962705612183] Epoch 7 [2560/13374] loss = 0.0018\n",
      "[5.539334774017334] Epoch 7 [5120/13374] loss = 0.0018\n",
      "[5.665376663208008] Epoch 7 [7680/13374] loss = 0.0018\n",
      "[5.792794704437256] Epoch 7 [10240/13374] loss = 0.0018\n",
      "[5.915395021438599] Epoch 7 [12800/13374] loss = 0.0018\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5541/6700 (82.70%)\n",
      "[6.200883626937866] Epoch 8 [2560/13374] loss = 0.0015\n",
      "[6.327461242675781] Epoch 8 [5120/13374] loss = 0.0016\n",
      "[6.453662633895874] Epoch 8 [7680/13374] loss = 0.0016\n",
      "[6.579859972000122] Epoch 8 [10240/13374] loss = 0.0016\n",
      "[6.707362174987793] Epoch 8 [12800/13374] loss = 0.0016\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5553/6700 (82.88%)\n",
      "[6.987091064453125] Epoch 9 [2560/13374] loss = 0.0014\n",
      "[7.115508794784546] Epoch 9 [5120/13374] loss = 0.0014\n",
      "[7.237990617752075] Epoch 9 [7680/13374] loss = 0.0015\n",
      "[7.36078667640686] Epoch 9 [10240/13374] loss = 0.0015\n",
      "[7.490599870681763] Epoch 9 [12800/13374] loss = 0.0015\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5607/6700 (83.69%)\n",
      "[7.770244598388672] Epoch 10 [2560/13374] loss = 0.0014\n",
      "[7.90054178237915] Epoch 10 [5120/13374] loss = 0.0014\n",
      "[8.032312393188477] Epoch 10 [7680/13374] loss = 0.0014\n",
      "[8.153110265731812] Epoch 10 [10240/13374] loss = 0.0014\n",
      "[8.273747682571411] Epoch 10 [12800/13374] loss = 0.0013\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5611/6700 (83.75%)\n",
      "[8.54818320274353] Epoch 11 [2560/13374] loss = 0.0012\n",
      "[8.665101289749146] Epoch 11 [5120/13374] loss = 0.0012\n",
      "[8.785041809082031] Epoch 11 [7680/13374] loss = 0.0012\n",
      "[8.908344268798828] Epoch 11 [10240/13374] loss = 0.0012\n",
      "[9.034337520599365] Epoch 11 [12800/13374] loss = 0.0012\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5639/6700 (84.16%)\n",
      "[9.317313432693481] Epoch 12 [2560/13374] loss = 0.0011\n",
      "[9.448498964309692] Epoch 12 [5120/13374] loss = 0.0011\n",
      "[9.584766626358032] Epoch 12 [7680/13374] loss = 0.0010\n",
      "[9.707021951675415] Epoch 12 [10240/13374] loss = 0.0011\n",
      "[9.834408521652222] Epoch 12 [12800/13374] loss = 0.0011\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5654/6700 (84.39%)\n",
      "[10.1090567111969] Epoch 13 [2560/13374] loss = 0.0009\n",
      "[10.23210859298706] Epoch 13 [5120/13374] loss = 0.0009\n",
      "[10.361398696899414] Epoch 13 [7680/13374] loss = 0.0010\n",
      "[10.48909592628479] Epoch 13 [10240/13374] loss = 0.0010\n",
      "[10.613147020339966] Epoch 13 [12800/13374] loss = 0.0010\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5652/6700 (84.36%)\n",
      "[10.892707586288452] Epoch 14 [2560/13374] loss = 0.0008\n",
      "[11.016953229904175] Epoch 14 [5120/13374] loss = 0.0008\n",
      "[11.148838520050049] Epoch 14 [7680/13374] loss = 0.0008\n",
      "[11.2828950881958] Epoch 14 [10240/13374] loss = 0.0009\n",
      "[11.40961217880249] Epoch 14 [12800/13374] loss = 0.0009\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5631/6700 (84.04%)\n",
      "[11.695358753204346] Epoch 15 [2560/13374] loss = 0.0007\n",
      "[11.825951099395752] Epoch 15 [5120/13374] loss = 0.0007\n",
      "[11.957770824432373] Epoch 15 [7680/13374] loss = 0.0008\n",
      "[12.086732387542725] Epoch 15 [10240/13374] loss = 0.0008\n",
      "[12.211212635040283] Epoch 15 [12800/13374] loss = 0.0008\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5637/6700 (84.13%)\n",
      "[12.49166488647461] Epoch 16 [2560/13374] loss = 0.0007\n",
      "[12.623355627059937] Epoch 16 [5120/13374] loss = 0.0007\n",
      "[12.75171184539795] Epoch 16 [7680/13374] loss = 0.0007\n",
      "[12.880143880844116] Epoch 16 [10240/13374] loss = 0.0007\n",
      "[13.00725531578064] Epoch 16 [12800/13374] loss = 0.0007\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5644/6700 (84.24%)\n",
      "[13.285105228424072] Epoch 17 [2560/13374] loss = 0.0006\n",
      "[13.41571593284607] Epoch 17 [5120/13374] loss = 0.0006\n",
      "[13.552230596542358] Epoch 17 [7680/13374] loss = 0.0006\n",
      "[13.679948806762695] Epoch 17 [10240/13374] loss = 0.0006\n",
      "[13.810338497161865] Epoch 17 [12800/13374] loss = 0.0006\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5632/6700 (84.06%)\n",
      "[14.09502100944519] Epoch 18 [2560/13374] loss = 0.0006\n",
      "[14.224043846130371] Epoch 18 [5120/13374] loss = 0.0006\n",
      "[14.350100040435791] Epoch 18 [7680/13374] loss = 0.0005\n",
      "[14.474892616271973] Epoch 18 [10240/13374] loss = 0.0005\n",
      "[14.602481365203857] Epoch 18 [12800/13374] loss = 0.0005\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5662/6700 (84.51%)\n",
      "[14.882031440734863] Epoch 19 [2560/13374] loss = 0.0004\n",
      "[15.010071277618408] Epoch 19 [5120/13374] loss = 0.0004\n",
      "[15.139872074127197] Epoch 19 [7680/13374] loss = 0.0004\n",
      "[15.270428895950317] Epoch 19 [10240/13374] loss = 0.0005\n",
      "[15.397824764251709] Epoch 19 [12800/13374] loss = 0.0005\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5646/6700 (84.27%)\n",
      "[15.681975841522217] Epoch 20 [2560/13374] loss = 0.0004\n",
      "[15.811111211776733] Epoch 20 [5120/13374] loss = 0.0004\n",
      "[15.940471172332764] Epoch 20 [7680/13374] loss = 0.0004\n",
      "[16.067912340164185] Epoch 20 [10240/13374] loss = 0.0004\n",
      "[16.201457738876343] Epoch 20 [12800/13374] loss = 0.0004\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5625/6700 (83.96%)\n",
      "[16.489060163497925] Epoch 21 [2560/13374] loss = 0.0004\n",
      "[16.626879453659058] Epoch 21 [5120/13374] loss = 0.0004\n",
      "[16.753056049346924] Epoch 21 [7680/13374] loss = 0.0004\n",
      "[16.88594913482666] Epoch 21 [10240/13374] loss = 0.0004\n",
      "[17.0141658782959] Epoch 21 [12800/13374] loss = 0.0004\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5620/6700 (83.88%)\n",
      "[17.3034086227417] Epoch 22 [2560/13374] loss = 0.0003\n",
      "[17.437793731689453] Epoch 22 [5120/13374] loss = 0.0003\n",
      "[17.56736993789673] Epoch 22 [7680/13374] loss = 0.0003\n",
      "[17.69462299346924] Epoch 22 [10240/13374] loss = 0.0003\n",
      "[17.827733039855957] Epoch 22 [12800/13374] loss = 0.0004\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5614/6700 (83.79%)\n",
      "[18.11013913154602] Epoch 23 [2560/13374] loss = 0.0003\n",
      "[18.237637519836426] Epoch 23 [5120/13374] loss = 0.0003\n",
      "[18.36220908164978] Epoch 23 [7680/13374] loss = 0.0003\n",
      "[18.492043018341064] Epoch 23 [10240/13374] loss = 0.0003\n",
      "[18.617026329040527] Epoch 23 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5635/6700 (84.10%)\n",
      "[18.918211698532104] Epoch 24 [2560/13374] loss = 0.0003\n",
      "[19.048428535461426] Epoch 24 [5120/13374] loss = 0.0003\n",
      "[19.177892208099365] Epoch 24 [7680/13374] loss = 0.0003\n",
      "[19.307051420211792] Epoch 24 [10240/13374] loss = 0.0003\n",
      "[19.434571266174316] Epoch 24 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5624/6700 (83.94%)\n",
      "[19.724782705307007] Epoch 25 [2560/13374] loss = 0.0003\n",
      "[19.86013674736023] Epoch 25 [5120/13374] loss = 0.0003\n",
      "[19.992040872573853] Epoch 25 [7680/13374] loss = 0.0003\n",
      "[20.126880645751953] Epoch 25 [10240/13374] loss = 0.0003\n",
      "[20.255695104599] Epoch 25 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5609/6700 (83.72%)\n",
      "[20.5368070602417] Epoch 26 [2560/13374] loss = 0.0002\n",
      "[20.663442134857178] Epoch 26 [5120/13374] loss = 0.0003\n",
      "[20.79348397254944] Epoch 26 [7680/13374] loss = 0.0003\n",
      "[20.92012619972229] Epoch 26 [10240/13374] loss = 0.0003\n",
      "[21.048515558242798] Epoch 26 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5617/6700 (83.84%)\n",
      "[21.335619688034058] Epoch 27 [2560/13374] loss = 0.0003\n",
      "[21.464935302734375] Epoch 27 [5120/13374] loss = 0.0003\n",
      "[21.5954909324646] Epoch 27 [7680/13374] loss = 0.0003\n",
      "[21.722081184387207] Epoch 27 [10240/13374] loss = 0.0003\n",
      "[21.848429203033447] Epoch 27 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5627/6700 (83.99%)\n",
      "[22.13375997543335] Epoch 28 [2560/13374] loss = 0.0002\n",
      "[22.25993275642395] Epoch 28 [5120/13374] loss = 0.0002\n",
      "[22.382623434066772] Epoch 28 [7680/13374] loss = 0.0002\n",
      "[22.504283905029297] Epoch 28 [10240/13374] loss = 0.0002\n",
      "[22.628809928894043] Epoch 28 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5615/6700 (83.81%)\n",
      "[22.90642213821411] Epoch 29 [2560/13374] loss = 0.0002\n",
      "[23.044028282165527] Epoch 29 [5120/13374] loss = 0.0002\n",
      "[23.16386866569519] Epoch 29 [7680/13374] loss = 0.0002\n",
      "[23.292640447616577] Epoch 29 [10240/13374] loss = 0.0002\n",
      "[23.42182946205139] Epoch 29 [12800/13374] loss = 0.0003\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5605/6700 (83.66%)\n",
      "[23.693987131118774] Epoch 30 [2560/13374] loss = 0.0002\n",
      "[23.818864345550537] Epoch 30 [5120/13374] loss = 0.0002\n",
      "[23.938333988189697] Epoch 30 [7680/13374] loss = 0.0002\n",
      "[24.060237169265747] Epoch 30 [10240/13374] loss = 0.0002\n",
      "[24.18998122215271] Epoch 30 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5627/6700 (83.99%)\n",
      "[24.46124505996704] Epoch 31 [2560/13374] loss = 0.0002\n",
      "[24.582963705062866] Epoch 31 [5120/13374] loss = 0.0002\n",
      "[24.711633682250977] Epoch 31 [7680/13374] loss = 0.0002\n",
      "[24.84616780281067] Epoch 31 [10240/13374] loss = 0.0002\n",
      "[24.972627878189087] Epoch 31 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5613/6700 (83.78%)\n",
      "[25.256656408309937] Epoch 32 [2560/13374] loss = 0.0002\n",
      "[25.38593292236328] Epoch 32 [5120/13374] loss = 0.0002\n",
      "[25.513989686965942] Epoch 32 [7680/13374] loss = 0.0002\n",
      "[25.643447637557983] Epoch 32 [10240/13374] loss = 0.0002\n",
      "[25.77336573600769] Epoch 32 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5623/6700 (83.93%)\n",
      "[26.05364203453064] Epoch 33 [2560/13374] loss = 0.0002\n",
      "[26.179583072662354] Epoch 33 [5120/13374] loss = 0.0002\n",
      "[26.300463438034058] Epoch 33 [7680/13374] loss = 0.0002\n",
      "[26.420040607452393] Epoch 33 [10240/13374] loss = 0.0002\n",
      "[26.544411420822144] Epoch 33 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5632/6700 (84.06%)\n",
      "[26.81737470626831] Epoch 34 [2560/13374] loss = 0.0002\n",
      "[26.939855575561523] Epoch 34 [5120/13374] loss = 0.0002\n",
      "[27.06467866897583] Epoch 34 [7680/13374] loss = 0.0002\n",
      "[27.190401554107666] Epoch 34 [10240/13374] loss = 0.0002\n",
      "[27.31171989440918] Epoch 34 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5616/6700 (83.82%)\n",
      "[27.58745312690735] Epoch 35 [2560/13374] loss = 0.0002\n",
      "[27.71289038658142] Epoch 35 [5120/13374] loss = 0.0002\n",
      "[27.848528146743774] Epoch 35 [7680/13374] loss = 0.0002\n",
      "[27.974392890930176] Epoch 35 [10240/13374] loss = 0.0002\n",
      "[28.105982303619385] Epoch 35 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5624/6700 (83.94%)\n",
      "[28.383223056793213] Epoch 36 [2560/13374] loss = 0.0002\n",
      "[28.507144689559937] Epoch 36 [5120/13374] loss = 0.0002\n",
      "[28.631865739822388] Epoch 36 [7680/13374] loss = 0.0002\n",
      "[28.760157346725464] Epoch 36 [10240/13374] loss = 0.0002\n",
      "[28.890743494033813] Epoch 36 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5624/6700 (83.94%)\n",
      "[29.17925477027893] Epoch 37 [2560/13374] loss = 0.0002\n",
      "[29.29817843437195] Epoch 37 [5120/13374] loss = 0.0002\n",
      "[29.42993450164795] Epoch 37 [7680/13374] loss = 0.0002\n",
      "[29.559189558029175] Epoch 37 [10240/13374] loss = 0.0002\n",
      "[29.690425395965576] Epoch 37 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5638/6700 (84.15%)\n",
      "[29.99454402923584] Epoch 38 [2560/13374] loss = 0.0002\n",
      "[30.126442193984985] Epoch 38 [5120/13374] loss = 0.0002\n",
      "[30.25545573234558] Epoch 38 [7680/13374] loss = 0.0002\n",
      "[30.379070520401] Epoch 38 [10240/13374] loss = 0.0002\n",
      "[30.50562572479248] Epoch 38 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5613/6700 (83.78%)\n",
      "[30.782447814941406] Epoch 39 [2560/13374] loss = 0.0002\n",
      "[30.91110587120056] Epoch 39 [5120/13374] loss = 0.0002\n",
      "[31.039196252822876] Epoch 39 [7680/13374] loss = 0.0002\n",
      "[31.166908025741577] Epoch 39 [10240/13374] loss = 0.0002\n",
      "[31.288373470306396] Epoch 39 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5632/6700 (84.06%)\n",
      "[31.572030067443848] Epoch 40 [2560/13374] loss = 0.0002\n",
      "[31.693986654281616] Epoch 40 [5120/13374] loss = 0.0002\n",
      "[31.813396215438843] Epoch 40 [7680/13374] loss = 0.0002\n",
      "[31.946325063705444] Epoch 40 [10240/13374] loss = 0.0002\n",
      "[32.070650815963745] Epoch 40 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5626/6700 (83.97%)\n",
      "[32.354196071624756] Epoch 41 [2560/13374] loss = 0.0002\n",
      "[32.484785318374634] Epoch 41 [5120/13374] loss = 0.0002\n",
      "[32.61044430732727] Epoch 41 [7680/13374] loss = 0.0002\n",
      "[32.73689389228821] Epoch 41 [10240/13374] loss = 0.0002\n",
      "[32.86294364929199] Epoch 41 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5625/6700 (83.96%)\n",
      "[33.13853073120117] Epoch 42 [2560/13374] loss = 0.0001\n",
      "[33.26362919807434] Epoch 42 [5120/13374] loss = 0.0002\n",
      "[33.392966985702515] Epoch 42 [7680/13374] loss = 0.0002\n",
      "[33.51821756362915] Epoch 42 [10240/13374] loss = 0.0002\n",
      "[33.64609408378601] Epoch 42 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5615/6700 (83.81%)\n",
      "[33.92498517036438] Epoch 43 [2560/13374] loss = 0.0002\n",
      "[34.05382513999939] Epoch 43 [5120/13374] loss = 0.0002\n",
      "[34.17726278305054] Epoch 43 [7680/13374] loss = 0.0002\n",
      "[34.30374479293823] Epoch 43 [10240/13374] loss = 0.0002\n",
      "[34.43471431732178] Epoch 43 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5619/6700 (83.87%)\n",
      "[34.70896315574646] Epoch 44 [2560/13374] loss = 0.0002\n",
      "[34.83572864532471] Epoch 44 [5120/13374] loss = 0.0002\n",
      "[34.96616339683533] Epoch 44 [7680/13374] loss = 0.0002\n",
      "[35.09313082695007] Epoch 44 [10240/13374] loss = 0.0002\n",
      "[35.22022080421448] Epoch 44 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5625/6700 (83.96%)\n",
      "[35.50334095954895] Epoch 45 [2560/13374] loss = 0.0001\n",
      "[35.631043910980225] Epoch 45 [5120/13374] loss = 0.0002\n",
      "[35.75648355484009] Epoch 45 [7680/13374] loss = 0.0002\n",
      "[35.885560274124146] Epoch 45 [10240/13374] loss = 0.0002\n",
      "[36.01189947128296] Epoch 45 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5634/6700 (84.09%)\n",
      "[36.28915309906006] Epoch 46 [2560/13374] loss = 0.0002\n",
      "[36.41548991203308] Epoch 46 [5120/13374] loss = 0.0002\n",
      "[36.5404748916626] Epoch 46 [7680/13374] loss = 0.0002\n",
      "[36.66829514503479] Epoch 46 [10240/13374] loss = 0.0002\n",
      "[36.79257130622864] Epoch 46 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5629/6700 (84.01%)\n",
      "[37.078864097595215] Epoch 47 [2560/13374] loss = 0.0002\n",
      "[37.207518100738525] Epoch 47 [5120/13374] loss = 0.0002\n",
      "[37.33263278007507] Epoch 47 [7680/13374] loss = 0.0002\n",
      "[37.4627730846405] Epoch 47 [10240/13374] loss = 0.0002\n",
      "[37.59270191192627] Epoch 47 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5617/6700 (83.84%)\n",
      "[37.86684727668762] Epoch 48 [2560/13374] loss = 0.0002\n",
      "[38.00033140182495] Epoch 48 [5120/13374] loss = 0.0001\n",
      "[38.128663301467896] Epoch 48 [7680/13374] loss = 0.0002\n",
      "[38.26477003097534] Epoch 48 [10240/13374] loss = 0.0002\n",
      "[38.394939661026] Epoch 48 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5623/6700 (83.93%)\n",
      "[38.69193363189697] Epoch 49 [2560/13374] loss = 0.0001\n",
      "[38.814950704574585] Epoch 49 [5120/13374] loss = 0.0001\n",
      "[38.943607568740845] Epoch 49 [7680/13374] loss = 0.0002\n",
      "[39.07204055786133] Epoch 49 [10240/13374] loss = 0.0002\n",
      "[39.197771072387695] Epoch 49 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5626/6700 (83.97%)\n",
      "[39.47987699508667] Epoch 50 [2560/13374] loss = 0.0001\n",
      "[39.60811114311218] Epoch 50 [5120/13374] loss = 0.0002\n",
      "[39.73804521560669] Epoch 50 [7680/13374] loss = 0.0002\n",
      "[39.86907505989075] Epoch 50 [10240/13374] loss = 0.0002\n",
      "[40.00156235694885] Epoch 50 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5638/6700 (84.15%)\n",
      "[40.282735109329224] Epoch 51 [2560/13374] loss = 0.0002\n",
      "[40.417815923690796] Epoch 51 [5120/13374] loss = 0.0002\n",
      "[40.55688118934631] Epoch 51 [7680/13374] loss = 0.0002\n",
      "[40.68751525878906] Epoch 51 [10240/13374] loss = 0.0002\n",
      "[40.807743549346924] Epoch 51 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5642/6700 (84.21%)\n",
      "[41.085256814956665] Epoch 52 [2560/13374] loss = 0.0002\n",
      "[41.211830377578735] Epoch 52 [5120/13374] loss = 0.0002\n",
      "[41.342599630355835] Epoch 52 [7680/13374] loss = 0.0002\n",
      "[41.466756105422974] Epoch 52 [10240/13374] loss = 0.0002\n",
      "[41.58789348602295] Epoch 52 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5646/6700 (84.27%)\n",
      "[41.86889171600342] Epoch 53 [2560/13374] loss = 0.0002\n",
      "[41.99662804603577] Epoch 53 [5120/13374] loss = 0.0002\n",
      "[42.13108682632446] Epoch 53 [7680/13374] loss = 0.0002\n",
      "[42.26102089881897] Epoch 53 [10240/13374] loss = 0.0002\n",
      "[42.388099670410156] Epoch 53 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5630/6700 (84.03%)\n",
      "[42.67258310317993] Epoch 54 [2560/13374] loss = 0.0001\n",
      "[42.804773569107056] Epoch 54 [5120/13374] loss = 0.0002\n",
      "[42.9325156211853] Epoch 54 [7680/13374] loss = 0.0002\n",
      "[43.057427406311035] Epoch 54 [10240/13374] loss = 0.0002\n",
      "[43.18618297576904] Epoch 54 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5643/6700 (84.22%)\n",
      "[43.47170567512512] Epoch 55 [2560/13374] loss = 0.0002\n",
      "[43.59592318534851] Epoch 55 [5120/13374] loss = 0.0001\n",
      "[43.723745822906494] Epoch 55 [7680/13374] loss = 0.0002\n",
      "[43.85493230819702] Epoch 55 [10240/13374] loss = 0.0002\n",
      "[43.98810291290283] Epoch 55 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5644/6700 (84.24%)\n",
      "[44.27402091026306] Epoch 56 [2560/13374] loss = 0.0001\n",
      "[44.40384483337402] Epoch 56 [5120/13374] loss = 0.0002\n",
      "[44.53478026390076] Epoch 56 [7680/13374] loss = 0.0002\n",
      "[44.66685366630554] Epoch 56 [10240/13374] loss = 0.0002\n",
      "[44.797844886779785] Epoch 56 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5629/6700 (84.01%)\n",
      "[45.08811545372009] Epoch 57 [2560/13374] loss = 0.0001\n",
      "[45.21467208862305] Epoch 57 [5120/13374] loss = 0.0001\n",
      "[45.35132360458374] Epoch 57 [7680/13374] loss = 0.0001\n",
      "[45.475377321243286] Epoch 57 [10240/13374] loss = 0.0002\n",
      "[45.605548620224] Epoch 57 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5627/6700 (83.99%)\n",
      "[45.90344548225403] Epoch 58 [2560/13374] loss = 0.0001\n",
      "[46.031548738479614] Epoch 58 [5120/13374] loss = 0.0002\n",
      "[46.16199278831482] Epoch 58 [7680/13374] loss = 0.0002\n",
      "[46.297765254974365] Epoch 58 [10240/13374] loss = 0.0002\n",
      "[46.424118518829346] Epoch 58 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5628/6700 (84.00%)\n",
      "[46.70924139022827] Epoch 59 [2560/13374] loss = 0.0001\n",
      "[46.83337092399597] Epoch 59 [5120/13374] loss = 0.0002\n",
      "[46.964335918426514] Epoch 59 [7680/13374] loss = 0.0002\n",
      "[47.08638525009155] Epoch 59 [10240/13374] loss = 0.0002\n",
      "[47.21277093887329] Epoch 59 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5639/6700 (84.16%)\n",
      "[47.496012449264526] Epoch 60 [2560/13374] loss = 0.0001\n",
      "[47.62142467498779] Epoch 60 [5120/13374] loss = 0.0001\n",
      "[47.7474730014801] Epoch 60 [7680/13374] loss = 0.0002\n",
      "[47.87618446350098] Epoch 60 [10240/13374] loss = 0.0002\n",
      "[48.00670313835144] Epoch 60 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5638/6700 (84.15%)\n",
      "[48.29710578918457] Epoch 61 [2560/13374] loss = 0.0001\n",
      "[48.42846345901489] Epoch 61 [5120/13374] loss = 0.0001\n",
      "[48.553988456726074] Epoch 61 [7680/13374] loss = 0.0001\n",
      "[48.68396973609924] Epoch 61 [10240/13374] loss = 0.0001\n",
      "[48.81362223625183] Epoch 61 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5628/6700 (84.00%)\n",
      "[49.1034722328186] Epoch 62 [2560/13374] loss = 0.0001\n",
      "[49.23365569114685] Epoch 62 [5120/13374] loss = 0.0001\n",
      "[49.36848020553589] Epoch 62 [7680/13374] loss = 0.0001\n",
      "[49.49611759185791] Epoch 62 [10240/13374] loss = 0.0002\n",
      "[49.624836921691895] Epoch 62 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5631/6700 (84.04%)\n",
      "[49.906819581985474] Epoch 63 [2560/13374] loss = 0.0001\n",
      "[50.03948760032654] Epoch 63 [5120/13374] loss = 0.0001\n",
      "[50.1643922328949] Epoch 63 [7680/13374] loss = 0.0001\n",
      "[50.296151638031006] Epoch 63 [10240/13374] loss = 0.0002\n",
      "[50.42828440666199] Epoch 63 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5644/6700 (84.24%)\n",
      "[50.71454668045044] Epoch 64 [2560/13374] loss = 0.0001\n",
      "[50.842453718185425] Epoch 64 [5120/13374] loss = 0.0001\n",
      "[50.97206377983093] Epoch 64 [7680/13374] loss = 0.0002\n",
      "[51.10167598724365] Epoch 64 [10240/13374] loss = 0.0002\n",
      "[51.23709058761597] Epoch 64 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5634/6700 (84.09%)\n",
      "[51.522496938705444] Epoch 65 [2560/13374] loss = 0.0001\n",
      "[51.65546178817749] Epoch 65 [5120/13374] loss = 0.0001\n",
      "[51.7865571975708] Epoch 65 [7680/13374] loss = 0.0001\n",
      "[51.914000511169434] Epoch 65 [10240/13374] loss = 0.0002\n",
      "[52.04013180732727] Epoch 65 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5597/6700 (83.54%)\n",
      "[52.32626390457153] Epoch 66 [2560/13374] loss = 0.0001\n",
      "[52.45547151565552] Epoch 66 [5120/13374] loss = 0.0002\n",
      "[52.58556389808655] Epoch 66 [7680/13374] loss = 0.0002\n",
      "[52.71126961708069] Epoch 66 [10240/13374] loss = 0.0002\n",
      "[52.84421110153198] Epoch 66 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5593/6700 (83.48%)\n",
      "[53.13654971122742] Epoch 67 [2560/13374] loss = 0.0002\n",
      "[53.265644550323486] Epoch 67 [5120/13374] loss = 0.0002\n",
      "[53.39781379699707] Epoch 67 [7680/13374] loss = 0.0002\n",
      "[53.524160861968994] Epoch 67 [10240/13374] loss = 0.0002\n",
      "[53.658615827560425] Epoch 67 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5622/6700 (83.91%)\n",
      "[53.94739317893982] Epoch 68 [2560/13374] loss = 0.0001\n",
      "[54.06948375701904] Epoch 68 [5120/13374] loss = 0.0002\n",
      "[54.19778823852539] Epoch 68 [7680/13374] loss = 0.0002\n",
      "[54.322999238967896] Epoch 68 [10240/13374] loss = 0.0002\n",
      "[54.45197772979736] Epoch 68 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5649/6700 (84.31%)\n",
      "[54.74945545196533] Epoch 69 [2560/13374] loss = 0.0001\n",
      "[54.876300573349] Epoch 69 [5120/13374] loss = 0.0001\n",
      "[55.00287389755249] Epoch 69 [7680/13374] loss = 0.0002\n",
      "[55.1345100402832] Epoch 69 [10240/13374] loss = 0.0002\n",
      "[55.26450777053833] Epoch 69 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5622/6700 (83.91%)\n",
      "[55.54723048210144] Epoch 70 [2560/13374] loss = 0.0001\n",
      "[55.67571306228638] Epoch 70 [5120/13374] loss = 0.0001\n",
      "[55.80091977119446] Epoch 70 [7680/13374] loss = 0.0001\n",
      "[55.928136587142944] Epoch 70 [10240/13374] loss = 0.0002\n",
      "[56.06041598320007] Epoch 70 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5625/6700 (83.96%)\n",
      "[56.347740650177] Epoch 71 [2560/13374] loss = 0.0001\n",
      "[56.47432541847229] Epoch 71 [5120/13374] loss = 0.0001\n",
      "[56.60401463508606] Epoch 71 [7680/13374] loss = 0.0001\n",
      "[56.73104286193848] Epoch 71 [10240/13374] loss = 0.0002\n",
      "[56.859440088272095] Epoch 71 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5640/6700 (84.18%)\n",
      "[57.14569664001465] Epoch 72 [2560/13374] loss = 0.0002\n",
      "[57.28136897087097] Epoch 72 [5120/13374] loss = 0.0002\n",
      "[57.407169818878174] Epoch 72 [7680/13374] loss = 0.0002\n",
      "[57.5322163105011] Epoch 72 [10240/13374] loss = 0.0002\n",
      "[57.6580114364624] Epoch 72 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5633/6700 (84.07%)\n",
      "[57.94317317008972] Epoch 73 [2560/13374] loss = 0.0001\n",
      "[58.07271957397461] Epoch 73 [5120/13374] loss = 0.0001\n",
      "[58.2004029750824] Epoch 73 [7680/13374] loss = 0.0002\n",
      "[58.32512807846069] Epoch 73 [10240/13374] loss = 0.0002\n",
      "[58.45524883270264] Epoch 73 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5631/6700 (84.04%)\n",
      "[58.73779058456421] Epoch 74 [2560/13374] loss = 0.0001\n",
      "[58.87018299102783] Epoch 74 [5120/13374] loss = 0.0001\n",
      "[59.00198173522949] Epoch 74 [7680/13374] loss = 0.0001\n",
      "[59.12787175178528] Epoch 74 [10240/13374] loss = 0.0002\n",
      "[59.261321783065796] Epoch 74 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5631/6700 (84.04%)\n",
      "[59.55681562423706] Epoch 75 [2560/13374] loss = 0.0001\n",
      "[59.68869709968567] Epoch 75 [5120/13374] loss = 0.0001\n",
      "[59.82024145126343] Epoch 75 [7680/13374] loss = 0.0002\n",
      "[59.94576334953308] Epoch 75 [10240/13374] loss = 0.0002\n",
      "[60.07671356201172] Epoch 75 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5648/6700 (84.30%)\n",
      "[60.35417938232422] Epoch 76 [2560/13374] loss = 0.0001\n",
      "[60.47738313674927] Epoch 76 [5120/13374] loss = 0.0001\n",
      "[60.609416484832764] Epoch 76 [7680/13374] loss = 0.0001\n",
      "[60.733909130096436] Epoch 76 [10240/13374] loss = 0.0001\n",
      "[60.867186307907104] Epoch 76 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5626/6700 (83.97%)\n",
      "[61.16229057312012] Epoch 77 [2560/13374] loss = 0.0002\n",
      "[61.28922200202942] Epoch 77 [5120/13374] loss = 0.0001\n",
      "[61.41409230232239] Epoch 77 [7680/13374] loss = 0.0001\n",
      "[61.540172815322876] Epoch 77 [10240/13374] loss = 0.0002\n",
      "[61.668259382247925] Epoch 77 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5661/6700 (84.49%)\n",
      "[61.95569825172424] Epoch 78 [2560/13374] loss = 0.0001\n",
      "[62.08334422111511] Epoch 78 [5120/13374] loss = 0.0001\n",
      "[62.211780309677124] Epoch 78 [7680/13374] loss = 0.0001\n",
      "[62.34787559509277] Epoch 78 [10240/13374] loss = 0.0001\n",
      "[62.47402858734131] Epoch 78 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5659/6700 (84.46%)\n",
      "[62.74738550186157] Epoch 79 [2560/13374] loss = 0.0001\n",
      "[62.87279176712036] Epoch 79 [5120/13374] loss = 0.0001\n",
      "[63.00260043144226] Epoch 79 [7680/13374] loss = 0.0001\n",
      "[63.13037872314453] Epoch 79 [10240/13374] loss = 0.0001\n",
      "[63.26823377609253] Epoch 79 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5628/6700 (84.00%)\n",
      "[63.54705238342285] Epoch 80 [2560/13374] loss = 0.0001\n",
      "[63.67823123931885] Epoch 80 [5120/13374] loss = 0.0001\n",
      "[63.804914712905884] Epoch 80 [7680/13374] loss = 0.0001\n",
      "[63.93449544906616] Epoch 80 [10240/13374] loss = 0.0001\n",
      "[64.06727528572083] Epoch 80 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5645/6700 (84.25%)\n",
      "[64.3513298034668] Epoch 81 [2560/13374] loss = 0.0001\n",
      "[64.4780023097992] Epoch 81 [5120/13374] loss = 0.0001\n",
      "[64.60862112045288] Epoch 81 [7680/13374] loss = 0.0001\n",
      "[64.73922967910767] Epoch 81 [10240/13374] loss = 0.0001\n",
      "[64.86756777763367] Epoch 81 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5639/6700 (84.16%)\n",
      "[65.16018295288086] Epoch 82 [2560/13374] loss = 0.0001\n",
      "[65.28976368904114] Epoch 82 [5120/13374] loss = 0.0001\n",
      "[65.41407704353333] Epoch 82 [7680/13374] loss = 0.0001\n",
      "[65.54044914245605] Epoch 82 [10240/13374] loss = 0.0001\n",
      "[65.67458868026733] Epoch 82 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5656/6700 (84.42%)\n",
      "[65.95503067970276] Epoch 83 [2560/13374] loss = 0.0001\n",
      "[66.08144927024841] Epoch 83 [5120/13374] loss = 0.0001\n",
      "[66.21182370185852] Epoch 83 [7680/13374] loss = 0.0001\n",
      "[66.34342002868652] Epoch 83 [10240/13374] loss = 0.0001\n",
      "[66.48139381408691] Epoch 83 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5642/6700 (84.21%)\n",
      "[66.76778054237366] Epoch 84 [2560/13374] loss = 0.0001\n",
      "[66.89824032783508] Epoch 84 [5120/13374] loss = 0.0001\n",
      "[67.03050661087036] Epoch 84 [7680/13374] loss = 0.0001\n",
      "[67.16411590576172] Epoch 84 [10240/13374] loss = 0.0001\n",
      "[67.28911018371582] Epoch 84 [12800/13374] loss = 0.0002\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5646/6700 (84.27%)\n",
      "[67.58203172683716] Epoch 85 [2560/13374] loss = 0.0001\n",
      "[67.71014332771301] Epoch 85 [5120/13374] loss = 0.0001\n",
      "[67.83932876586914] Epoch 85 [7680/13374] loss = 0.0001\n",
      "[67.96809339523315] Epoch 85 [10240/13374] loss = 0.0001\n",
      "[68.09804844856262] Epoch 85 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5633/6700 (84.07%)\n",
      "[68.38715624809265] Epoch 86 [2560/13374] loss = 0.0001\n",
      "[68.51616668701172] Epoch 86 [5120/13374] loss = 0.0001\n",
      "[68.6527771949768] Epoch 86 [7680/13374] loss = 0.0001\n",
      "[68.78589797019958] Epoch 86 [10240/13374] loss = 0.0001\n",
      "[68.91486382484436] Epoch 86 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5644/6700 (84.24%)\n",
      "[69.19646644592285] Epoch 87 [2560/13374] loss = 0.0001\n",
      "[69.32893896102905] Epoch 87 [5120/13374] loss = 0.0001\n",
      "[69.46107816696167] Epoch 87 [7680/13374] loss = 0.0001\n",
      "[69.59634828567505] Epoch 87 [10240/13374] loss = 0.0001\n",
      "[69.7237343788147] Epoch 87 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5642/6700 (84.21%)\n",
      "[70.00925636291504] Epoch 88 [2560/13374] loss = 0.0001\n",
      "[70.14164781570435] Epoch 88 [5120/13374] loss = 0.0001\n",
      "[70.26567077636719] Epoch 88 [7680/13374] loss = 0.0001\n",
      "[70.39929294586182] Epoch 88 [10240/13374] loss = 0.0001\n",
      "[70.53380560874939] Epoch 88 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5631/6700 (84.04%)\n",
      "[70.81400346755981] Epoch 89 [2560/13374] loss = 0.0001\n",
      "[70.93297934532166] Epoch 89 [5120/13374] loss = 0.0001\n",
      "[71.05478572845459] Epoch 89 [7680/13374] loss = 0.0001\n",
      "[71.17729568481445] Epoch 89 [10240/13374] loss = 0.0001\n",
      "[71.31202363967896] Epoch 89 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5652/6700 (84.36%)\n",
      "[71.58933019638062] Epoch 90 [2560/13374] loss = 0.0001\n",
      "[71.71740937232971] Epoch 90 [5120/13374] loss = 0.0001\n",
      "[71.85067677497864] Epoch 90 [7680/13374] loss = 0.0001\n",
      "[71.97780895233154] Epoch 90 [10240/13374] loss = 0.0001\n",
      "[72.10626554489136] Epoch 90 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5648/6700 (84.30%)\n",
      "[72.38560628890991] Epoch 91 [2560/13374] loss = 0.0001\n",
      "[72.51789903640747] Epoch 91 [5120/13374] loss = 0.0001\n",
      "[72.64965581893921] Epoch 91 [7680/13374] loss = 0.0001\n",
      "[72.78508400917053] Epoch 91 [10240/13374] loss = 0.0001\n",
      "[72.91716027259827] Epoch 91 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5640/6700 (84.18%)\n",
      "[73.20229578018188] Epoch 92 [2560/13374] loss = 0.0001\n",
      "[73.33001708984375] Epoch 92 [5120/13374] loss = 0.0001\n",
      "[73.45687961578369] Epoch 92 [7680/13374] loss = 0.0001\n",
      "[73.58667206764221] Epoch 92 [10240/13374] loss = 0.0001\n",
      "[73.71635866165161] Epoch 92 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5639/6700 (84.16%)\n",
      "[74.00029826164246] Epoch 93 [2560/13374] loss = 0.0001\n",
      "[74.13266682624817] Epoch 93 [5120/13374] loss = 0.0001\n",
      "[74.26368713378906] Epoch 93 [7680/13374] loss = 0.0001\n",
      "[74.39146757125854] Epoch 93 [10240/13374] loss = 0.0001\n",
      "[74.51776766777039] Epoch 93 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5646/6700 (84.27%)\n",
      "[74.79666900634766] Epoch 94 [2560/13374] loss = 0.0001\n",
      "[74.92263650894165] Epoch 94 [5120/13374] loss = 0.0001\n",
      "[75.0545973777771] Epoch 94 [7680/13374] loss = 0.0001\n",
      "[75.18647027015686] Epoch 94 [10240/13374] loss = 0.0001\n",
      "[75.31265687942505] Epoch 94 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5641/6700 (84.19%)\n",
      "[75.59216213226318] Epoch 95 [2560/13374] loss = 0.0001\n",
      "[75.72262740135193] Epoch 95 [5120/13374] loss = 0.0001\n",
      "[75.85553121566772] Epoch 95 [7680/13374] loss = 0.0001\n",
      "[75.98027682304382] Epoch 95 [10240/13374] loss = 0.0001\n",
      "[76.1083607673645] Epoch 95 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5654/6700 (84.39%)\n",
      "[76.39235305786133] Epoch 96 [2560/13374] loss = 0.0001\n",
      "[76.51713514328003] Epoch 96 [5120/13374] loss = 0.0001\n",
      "[76.64343094825745] Epoch 96 [7680/13374] loss = 0.0001\n",
      "[76.76820254325867] Epoch 96 [10240/13374] loss = 0.0001\n",
      "[76.9003791809082] Epoch 96 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5658/6700 (84.45%)\n",
      "[77.1706817150116] Epoch 97 [2560/13374] loss = 0.0001\n",
      "[77.30513787269592] Epoch 97 [5120/13374] loss = 0.0001\n",
      "[77.43442559242249] Epoch 97 [7680/13374] loss = 0.0001\n",
      "[77.55877780914307] Epoch 97 [10240/13374] loss = 0.0001\n",
      "[77.68345308303833] Epoch 97 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5635/6700 (84.10%)\n",
      "[77.96894884109497] Epoch 98 [2560/13374] loss = 0.0001\n",
      "[78.0997200012207] Epoch 98 [5120/13374] loss = 0.0001\n",
      "[78.23284649848938] Epoch 98 [7680/13374] loss = 0.0001\n",
      "[78.35759806632996] Epoch 98 [10240/13374] loss = 0.0001\n",
      "[78.48587775230408] Epoch 98 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5647/6700 (84.28%)\n",
      "[78.76640748977661] Epoch 99 [2560/13374] loss = 0.0001\n",
      "[78.89128041267395] Epoch 99 [5120/13374] loss = 0.0001\n",
      "[79.01849889755249] Epoch 99 [7680/13374] loss = 0.0001\n",
      "[79.1514220237732] Epoch 99 [10240/13374] loss = 0.0001\n",
      "[79.28064751625061] Epoch 99 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5638/6700 (84.15%)\n",
      "[79.5871787071228] Epoch 100 [2560/13374] loss = 0.0001\n",
      "[79.71453309059143] Epoch 100 [5120/13374] loss = 0.0001\n",
      "[79.84276270866394] Epoch 100 [7680/13374] loss = 0.0001\n",
      "[79.97114253044128] Epoch 100 [10240/13374] loss = 0.0001\n",
      "[80.09182143211365] Epoch 100 [12800/13374] loss = 0.0001\n",
      "Evaluating trained model...\n",
      "Test set: Accuracy 5649/6700 (84.31%)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "\n",
    "    USE_GPU = True\n",
    "\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    acc_list = []\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch1 = np.arange(1,len(acc_list)+1,1)\n",
    "acc_list1=np.array(acc_list)\n",
    "plt.plot(epoch1,acc_list1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldeepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
