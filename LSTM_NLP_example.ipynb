{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_box = ['e', 'h', 'l', 'o']\n",
    "char_hello = [1, 0, 2, 2, 3]\n",
    "char_ohlol = [3, 1, 2, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(char_hello).view(5,1) # input size = 5\n",
    "targets = torch.tensor(char_ohlol) # for CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inputs (5,1) 因为序列长度为5, batch为1, 即RNN序列的每一个单元，都有batch_size个输入同时进行输入。input_size为4因为在inputs中有四类字母\n",
    "#### embedding后 embedding (5,1,10) 即对于RNN序列的每一个单元我都有一个dim=10的向量作为输入。即embedding_size = 10\n",
    "#### 经过一个hidden后，hidden (5,1,4), hidden_size = 4，同样是因为输出的character种类有四种，经过softmax层会输出四种character的概率，所以hidden_size是4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3562,  0.0273,  0.7150,  0.3079],\n",
       "        [ 0.3312,  0.6649,  0.4634, -0.4636],\n",
       "        [ 0.4077,  0.0918, -0.6117, -0.3159],\n",
       "        [ 0.4445, -0.1631, -0.8916, -0.2115],\n",
       "        [-0.1126,  0.1363, -0.6604,  0.1144]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, inputs): # 输入的inputs是一个(5,1), RNN中的inputs应该是(seq_len, batch_size, input_size)\n",
    "        inputs = self.emb(inputs)\n",
    "        \n",
    "        # # RNN hn的形状(num_layers, batch_size, hidden_size)\n",
    "        # hn = torch.zeros(num_layers, inputs.size(1), hidden_size)\n",
    "        # outputs, hn = self.rnn(inputs, hn) # outputs的形状应该是(seq_len, batch_size, hidden_size)\n",
    "\n",
    "        # # LSTM 输出有两个hn和cn，初始化和前向传播的时候要注意\n",
    "        # hn = torch.zeros(num_layers, inputs.size(1), hidden_size) # LSTM hn的形状与 RNN相同(num_layers, batch_size, hidden_size)\n",
    "        # cn = torch.zeros(num_layers, inputs.size(1), hidden_size) # LSTM cn的形状与其对应的hn的形状相同，因为要进行对应位置元素的相乘(num_layers, batch_size, hidden_size)\n",
    "        # outputs, (hn, cn) = self.lstm(inputs, (hn, cn)) \n",
    "\n",
    "        # GRU 门控循环单元，使用方法与RNN完全相同\n",
    "        hn = torch.zeros(num_layers, inputs.size(1), hidden_size)\n",
    "        outputs, hn = self.gru(inputs, hn)\n",
    "\n",
    "        return outputs.view(-1,4) #(5,1,4)->(5,4)\n",
    "\n",
    "input_size = 4 # 4类character 'e', 'h', 'l', 'o'\n",
    "embedding_size = 10\n",
    "hidden_size = 4 # 4类character 'e', 'h', 'l', 'o'\n",
    "num_layers = 1\n",
    "\n",
    "mynet = MyNet(input_size=input_size, embedding_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "mynet.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------第1轮训练开始-----------\n",
      "lheeh\n",
      "第1轮训练结束，loss=1.5876580476760864\n",
      "-----------第2轮训练开始-----------\n",
      "lheeo\n",
      "第2轮训练结束，loss=1.2646535634994507\n",
      "-----------第3轮训练开始-----------\n",
      "lhool\n",
      "第3轮训练结束，loss=1.0389230251312256\n",
      "-----------第4轮训练开始-----------\n",
      "olool\n",
      "第4轮训练结束，loss=0.8855615854263306\n",
      "-----------第5轮训练开始-----------\n",
      "ollol\n",
      "第5轮训练结束，loss=0.7852229475975037\n",
      "-----------第6轮训练开始-----------\n",
      "ollol\n",
      "第6轮训练结束，loss=0.7265177369117737\n",
      "-----------第7轮训练开始-----------\n",
      "ollol\n",
      "第7轮训练结束，loss=0.6921847462654114\n",
      "-----------第8轮训练开始-----------\n",
      "ohlol\n",
      "第8轮训练结束，loss=0.6697393655776978\n",
      "-----------第9轮训练开始-----------\n",
      "ohlol\n",
      "第9轮训练结束，loss=0.6537488698959351\n",
      "-----------第10轮训练开始-----------\n",
      "ohlol\n",
      "第10轮训练结束，loss=0.6418073177337646\n",
      "-----------第11轮训练开始-----------\n",
      "ohlol\n",
      "第11轮训练结束，loss=0.6322439908981323\n",
      "-----------第12轮训练开始-----------\n",
      "ohlol\n",
      "第12轮训练结束，loss=0.623809814453125\n",
      "-----------第13轮训练开始-----------\n",
      "ohlol\n",
      "第13轮训练结束，loss=0.6157842874526978\n",
      "-----------第14轮训练开始-----------\n",
      "ohlol\n",
      "第14轮训练结束，loss=0.6078770160675049\n",
      "-----------第15轮训练开始-----------\n",
      "ohlol\n",
      "第15轮训练结束，loss=0.6000838875770569\n",
      "-----------第16轮训练开始-----------\n",
      "ohlol\n",
      "第16轮训练结束，loss=0.5924297571182251\n",
      "-----------第17轮训练开始-----------\n",
      "ohlol\n",
      "第17轮训练结束，loss=0.5844842791557312\n",
      "-----------第18轮训练开始-----------\n",
      "ohlol\n",
      "第18轮训练结束，loss=0.5752609968185425\n",
      "-----------第19轮训练开始-----------\n",
      "ohlol\n",
      "第19轮训练结束，loss=0.5642666220664978\n",
      "-----------第20轮训练开始-----------\n",
      "ohlol\n",
      "第20轮训练结束，loss=0.5524497032165527\n",
      "-----------第21轮训练开始-----------\n",
      "ohlol\n",
      "第21轮训练结束，loss=0.5417803525924683\n",
      "-----------第22轮训练开始-----------\n",
      "ohlol\n",
      "第22轮训练结束，loss=0.5339158177375793\n",
      "-----------第23轮训练开始-----------\n",
      "ohlol\n",
      "第23轮训练结束，loss=0.5291659235954285\n",
      "-----------第24轮训练开始-----------\n",
      "ohlol\n",
      "第24轮训练结束，loss=0.5267678499221802\n",
      "-----------第25轮训练开始-----------\n",
      "ohlol\n",
      "第25轮训练结束，loss=0.5257364511489868\n",
      "-----------第26轮训练开始-----------\n",
      "ohlol\n",
      "第26轮训练结束，loss=0.5252690315246582\n",
      "-----------第27轮训练开始-----------\n",
      "ohlol\n",
      "第27轮训练结束，loss=0.5245916247367859\n",
      "-----------第28轮训练开始-----------\n",
      "ohlol\n",
      "第28轮训练结束，loss=0.5228258371353149\n",
      "-----------第29轮训练开始-----------\n",
      "ohlol\n",
      "第29轮训练结束，loss=0.5195519328117371\n",
      "-----------第30轮训练开始-----------\n",
      "ohlol\n",
      "第30轮训练结束，loss=0.5153011083602905\n"
     ]
    }
   ],
   "source": [
    "epoch = 30\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(mynet.parameters(), lr=0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(epoch):\n",
    "        print(f'-----------第{i+1}轮训练开始-----------')\n",
    "        mynet.train()\n",
    "\n",
    "        outputs = mynet(inputs)\n",
    "        result = outputs.argmax(axis=1) # argmax()只能处理二维矩阵\n",
    "        for num in result:\n",
    "            print(char_box[num], end=\"\") #输出结果\n",
    "        print() # 换行\n",
    "        \n",
    "        loss = loss_fn(outputs, targets) # outputs的形状必须是(N,C), target的形状是(N,)的一维数组。N代表seq_len, C代表用于分类的种类\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(f'第{i+1}轮训练结束，loss={loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soft_robot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
